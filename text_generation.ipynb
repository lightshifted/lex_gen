{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03989292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# nvidia-smi that clear gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# import transformers utils\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2ForSequenceClassification, GPT2TokenizerFast\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View GPU memory\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca67226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract information from .vtt files\n",
    "import webvtt\n",
    "\n",
    "def convert_vtt(filenames):\n",
    "    # create asset folder if one doesn't already exist\n",
    "    if os.path.isdir('{}/text'.format(os.getcwd())) == False:\n",
    "        os.makedirs('text')\n",
    "    # extract the text and times from the vtt file\n",
    "    for file in filenames:\n",
    "        captions = webvtt.read(file)\n",
    "        text_time = pd.DataFrame()\n",
    "        text_time['text'] = [caption.text for caption in captions]\n",
    "        text_time['start'] = [caption.start for caption in captions]\n",
    "        text_time['stop'] = [caption.end for caption in captions]\n",
    "        text_time.to_csv('transcripts/text/{}.csv'.format(file[18:-4]), index=False) # -4 to remove '.vtt'\n",
    "        # remove files from local drive\n",
    "        os.remove(file)\n",
    "        \n",
    "        \n",
    "def prepare_data(return_text=False, save_text=True, vtt=False, path='transcripts/corpus.txt', num_samples:int = 10):\n",
    "    from glob import glob\n",
    "    paths = glob(\"./transcripts/vtt/*_large.vtt\", recursive=True)\n",
    "    if vtt:\n",
    "        convert_vtt(paths)\n",
    "    \n",
    "    csv_files = [os.fsdecode(file) for file in os.listdir('transcripts/text') if os.fsdecode(file).endswith('.csv')]\n",
    "    texts = []\n",
    "    \n",
    "    for file in csv_files[:num_samples]:\n",
    "        df = pd.read_csv('transcripts/text/' + file)\n",
    "        text = \"\".join(df.text)\n",
    "        texts.append(text)\n",
    "        \n",
    "    if save_text:\n",
    "        with open(path, 'w') as fp:\n",
    "            fp.write(texts) # <== This line raises error warning. Fix it.\n",
    "\n",
    "    if return_text:\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912536f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c015288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather file names of large vtt files only \n",
    "from glob import glob\n",
    "paths = glob(\"./transcripts/text/*.csv\", recursive=True)\n",
    "# convert_vtt(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a873701",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [os.fsdecode(file) for file in os.listdir('transcripts/text') if os.fsdecode(file).endswith('.csv')]\n",
    "#take a look at a file name\n",
    "csv_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cc6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text\n",
    "texts = []\n",
    "for file in csv_files[:100]:\n",
    "    df = pd.read_csv('transcripts/text/' + file)\n",
    "    text = \"\".join(df.text)\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aab73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "# Instantiate tokenizer and pass `gpt2` to the `from_pretrained` method \n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Select token to uses as `pad_token`\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Default to right padding\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Set context length\n",
    "context_length = 128\n",
    "\n",
    "# Process text\n",
    "inputs = tokenizer(texts, \n",
    "                   padding='longest',\n",
    "                   truncation=True,\n",
    "                   return_tensors=\"pt\",\n",
    "                   max_length=context_length, # context size GPT-2: 1,024, GPT-3: 2,048\n",
    "                   return_overflowing_tokens=True, # tokenize input and split into chunks\n",
    "                   return_length=True, # return length of each created chunk\n",
    "                   return_special_tokens_mask=True\n",
    "                  )\n",
    "\n",
    "print(f\"Input IDs length: {len(inputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(inputs['length'])}\")\n",
    "print(f\"Chunk mapping: {inputs['overflow_to_sample_mapping']}\")\n",
    "print(\"input_ids\\n\", inputs['input_ids'][0])\n",
    "print(\"attention_mask\\n\", inputs['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoads(Dataset):\n",
    "    \n",
    "    def __init__(self, X, Mask):\n",
    "        self.x = X\n",
    "        self.mask = Mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids':self.x[idx],\n",
    "            'attention_mask':self.mask[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5496718",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = int(0.8*len(inputs['input_ids']))\n",
    "n2 = int(0.9*len(inputs['input_ids']))\n",
    "         \n",
    "Xtr = inputs['input_ids'][:n1]\n",
    "tr_mask = inputs['attention_mask'][:n1]\n",
    "\n",
    "Xdev = inputs['input_ids'][n1:n2]\n",
    "dev_mask = inputs['attention_mask'][n1:n2]\n",
    "\n",
    "Xte = inputs['input_ids'][n2:]\n",
    "te_mask = inputs['attention_mask'][n2:]\n",
    "\n",
    "print(f\"{Xtr.shape=}\")\n",
    "print(f\"{Xdev.shape=}\")\n",
    "print(f\"{Xte.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a959c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for input to transformer\n",
    "tr_loader = DataLoads(Xtr, tr_mask)\n",
    "dev_loader = DataLoads(Xdev, dev_mask)\n",
    "\n",
    "trainset = DataLoader(tr_loader, shuffle=True, batch_size=3)\n",
    "devset = DataLoader(dev_loader, shuffle=False, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\", vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"The model has {model_size/1000**2:.1f}M parameters to tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208209a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Loop\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"artifacts\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=10,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=200,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "tokenizer=tokenizer,\n",
    "args=args,\n",
    "data_collator=data_collator,\n",
    "train_dataset=tr_loader,\n",
    "eval_dataset=dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71aa855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "# How well does my trained model actually perform?\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "\"text-generation\", model=\"artifacts/checkpoint-200\")\n",
    "\n",
    "txt = \"Artificial Intelligence will\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1149e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Artificial Intelligence will\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80769738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Advanced Training Loop ================\n",
    "# Since I'm primarily interested in autocompletion for prompts around meaning,\n",
    "# I will give more weight to training samples with \"meaning\" and its synonyms.\n",
    "# Let's check for these words' existence in the tokenizer vocabulary\n",
    "\n",
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"connotation\",\n",
    "    \"content\",\n",
    "    \"context\",\n",
    "    \"definition\",\n",
    "    \"effect\",\n",
    "    \"essence\",\n",
    "    \"explanation\",\n",
    "    \"hint\",\n",
    "    \"implication\",\n",
    "    \"interpretation\",\n",
    "    \"nuance\",\n",
    "    \"sense\",\n",
    "    \"significance\",\n",
    "    \"spirit\",\n",
    "    \"purpose\",\n",
    "    \"direction\",\n",
    "    \"subject\",\n",
    "    \"substance\",\n",
    "    \"understanding\",\n",
    "    \"value\",\n",
    "    \"intention\",\n",
    "    \"aim\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfef24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You need to study the below code. You initially copied and pasted it into the notebook cell. \n",
    "### It's value is such that closer inspection is warranted, and will pay dividends later in the form\n",
    "### of knowledge which can be applied to future problems.\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "        axis=[0, 2]\n",
    "    )\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b748eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tr_loader, batch_size=32, shuffle=True)\n",
    "eval_dataloader = DataLoader(dev_loader, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a90c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bca8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(fp16=True)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8324c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 5000\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "#                     \"lr\": get_lr(),\n",
    "#                     \"samples\": step * samples_per_step,\n",
    "#                     \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                repo.push_to_hub(\n",
    "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547778a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "\"text-generation\", model=\"artifacts/checkpoint-200\")\n",
    "\n",
    "txt = \"Artificial intelligence\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d77b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6af95c5ed43ff950dc273fb358a3c7543a579024e645fcf2892b4462426f781d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
