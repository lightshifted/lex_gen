{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3670a49",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03989292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# import transformers utils\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2ForSequenceClassification, GPT2TokenizerFast\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# nvidia-smi that clears gpu memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555f16b",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136f2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operations.utils import read_text\n",
    "\n",
    "def read_text(path:str=\"./transcripts/corpus\", file_name: str=\"corpus.txt\"):\n",
    "    with open(f'{path}/{file_name}', 'r', encoding='utf-8') as f:\n",
    "        texts = f.read()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b6154a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/30/22 10:22:58] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Input IDs length: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32583</span>                         <a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1229640824.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py#30\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/30/22 10:22:58]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Input IDs length: \u001b[1;36m32583\u001b[0m                         \u001b]8;id=377791;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py\u001b\\\u001b[2m1229640824.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=998416;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py#30\u001b\\\u001b[2m30\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Input chunk lengths: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>,     <a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1229640824.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py#31\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">])</span>                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Input chunk lengths: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m256\u001b[0m, \u001b[1;36m256\u001b[0m, \u001b[1;36m256\u001b[0m,     \u001b]8;id=311103;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py\u001b\\\u001b[2m1229640824.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=893235;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py#31\u001b\\\u001b[2m31\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[33m...\u001b[0m, \u001b[1;36m256\u001b[0m, \u001b[1;36m256\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m                            \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Chunk mapping: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">])</span> <a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1229640824.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py#32\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Chunk mapping: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m \u001b]8;id=804317;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py\u001b\\\u001b[2m1229640824.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=44041;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\1229640824.py#32\u001b\\\u001b[2m32\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize\n",
    "from operations.data import tokenize_text\n",
    "from config.config import logger\n",
    "\n",
    "def tokenize_text(text, model_name: str=\"gpt2\", padding_side: str=\"right\", context_length: int=256, tokenizer_only=False):\n",
    "    \n",
    "    # Instantiate tokenizer and pass `gpt2` to the `from_pretrained` method \n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "    # Select token to uses as `pad_token`\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Default to right padding\n",
    "    tokenizer.padding_side = padding_side\n",
    "    # Set context length\n",
    "    context_length = context_length\n",
    "    \n",
    "    if tokenizer_only == True:\n",
    "        return tokenizer\n",
    "    \n",
    "    # Process text\n",
    "    inputs = tokenizer(text, \n",
    "                       padding='longest',\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\",\n",
    "                       max_length=context_length, # context size GPT-2: 1,024, GPT-3: 2,048\n",
    "                       return_overflowing_tokens=True, # tokenize input and split into chunks\n",
    "                       return_length=True, # return length of each created chunk\n",
    "                       return_special_tokens_mask=True\n",
    "                      )\n",
    "    \n",
    "    logger.info(f\"Input IDs length: {len(inputs['input_ids'])}\")\n",
    "    logger.info(f\"Input chunk lengths: {(inputs['length'])}\")\n",
    "    logger.info(f\"Chunk mapping: {inputs['overflow_to_sample_mapping']}\")\n",
    "\n",
    "    return inputs\n",
    "    \n",
    "inputs = tokenize_text(read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c31bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/30/22 10:23:01] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Xtr.<span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26066</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">])</span>              <a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2131086292.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2131086292.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2131086292.py#17\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/30/22 10:23:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Xtr.\u001b[33mshape\u001b[0m=\u001b[1;35mtorch\u001b[0m\u001b[1;35m.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m26066\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m              \u001b]8;id=899041;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2131086292.py\u001b\\\u001b[2m2131086292.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=817867;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2131086292.py#17\u001b\\\u001b[2m17\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Xval.<span style=\"color: #808000; text-decoration-color: #808000\">shape</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3258</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"font-weight: bold\">])</span>              <a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2131086292.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2131086292.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2131086292.py#18\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Xval.\u001b[33mshape\u001b[0m=\u001b[1;35mtorch\u001b[0m\u001b[1;35m.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m3258\u001b[0m, \u001b[1;36m256\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m              \u001b]8;id=793487;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2131086292.py\u001b\\\u001b[2m2131086292.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=881090;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2131086292.py#18\u001b\\\u001b[2m18\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train/Dev split\n",
    "from typing import List, Dict\n",
    "from operations.data import data_split\n",
    "\n",
    "def data_split(tokens: Dict , train_split: float=0.8, validation_split: float=0.9):\n",
    "    assert \"input_ids\" and \"attention_mask\" in tokens.keys()\n",
    "    \n",
    "    n1 = int(train_split * len(tokens['input_ids']))\n",
    "    n2 = int(validation_split * len(tokens['input_ids']))\n",
    "    \n",
    "    Xtr = tokens['input_ids'][:n1]\n",
    "    tr_mask = tokens['attention_mask'][:n1]\n",
    "    \n",
    "    Xval = tokens['input_ids'][n1:n2]\n",
    "    val_mask = tokens['attention_mask'][n1:n2]\n",
    "    \n",
    "    logger.info(f\"{Xtr.shape=}\")\n",
    "    logger.info(f\"{Xval.shape=}\")\n",
    "    \n",
    "    return Xtr, tr_mask, Xval, val_mask\n",
    "\n",
    "\n",
    "Xtr, tr_mask, Xval, val_mask = data_split(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185d161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Staging\n",
    "# from operations.data import DataLoads\n",
    "\n",
    "class DataLoads(Dataset):\n",
    "    \n",
    "    def __init__(self, X, Mask):\n",
    "        self.x = X\n",
    "        self.mask = Mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids':self.x[idx],\n",
    "            'attention_mask':self.mask[idx],\n",
    "        }\n",
    "    \n",
    "def stage_data(xtrain: torch.tensor, \n",
    "               xtrain_mask: torch.tensor, \n",
    "               xval: torch.tensor, \n",
    "               xval_mask: torch.tensor):\n",
    "    \n",
    "    train_loader = DataLoads(xtrain, xtrain_mask)\n",
    "    val_loader = DataLoads(xval, xval_mask)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a959c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for input to transformer\n",
    "tr_loader, dev_loader = stage_data(Xtr, tr_mask, Xval, val_mask)\n",
    "\n",
    "trainset = DataLoader(tr_loader, shuffle=True, batch_size=3)\n",
    "devset = DataLoader(dev_loader, shuffle=False, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e3fb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/30/22 10:25:54] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PreTrainedTokenizerFast</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name_or_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt2'</span>,    <a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2645677773.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2645677773.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2645677773.py#12\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50257</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_max_len</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>,           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #808000; text-decoration-color: #808000\">is_fast</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>,             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #808000; text-decoration-color: #808000\">truncation_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>,                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #808000; text-decoration-color: #808000\">special_tokens</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>,   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span>:      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pad_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"font-weight: bold\">})</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/30/22 10:25:54]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;35mPreTrainedTokenizerFast\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'gpt2'\u001b[0m,    \u001b]8;id=409574;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2645677773.py\u001b\\\u001b[2m2645677773.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=398606;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\2645677773.py#12\u001b\\\u001b[2m12\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[33mvocab_size\u001b[0m=\u001b[1;36m50257\u001b[0m, \u001b[33mmodel_max_len\u001b[0m=\u001b[1;36m1024\u001b[0m,           \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[33mis_fast\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'right'\u001b[0m,             \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[33mtruncation_side\u001b[0m=\u001b[32m'right'\u001b[0m,                        \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m,   \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'eos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'unk_token'\u001b[0m:      \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'pad_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "def get_tokenizer(model_name: str=\"gpt2\", padding_side:str=\"right\"):\n",
    "    # Instantiate tokenizer and pass `gpt2` to the `from_pretrained` method \n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "    # Select token to uses as `pad_token`\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Default to right padding\n",
    "    tokenizer.padding_side = padding_side\n",
    "    \n",
    "    logger.info(tokenizer)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "context_length = 256\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\", vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30725e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/30/22 10:23:34] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> The model has <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">124.</span>4M parameters to tune          <a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\4012617785.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4012617785.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\4012617785.py#3\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/30/22 10:23:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m The model has \u001b[1;36m124.\u001b[0m4M parameters to tune          \u001b]8;id=512693;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\4012617785.py\u001b\\\u001b[2m4012617785.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=41555;file://C:\\Users\\Hedronstone\\AppData\\Local\\Temp\\ipykernel_4676\\4012617785.py#3\u001b\\\u001b[2m3\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\",config=config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "logger.info(f\"The model has {model_size/1000**2:.1f}M parameters to tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208209a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Loop\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"artifacts\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=10,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=200,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "tokenizer=tokenizer,\n",
    "args=args,\n",
    "data_collator=data_collator,\n",
    "train_dataset=tr_loader,\n",
    "eval_dataset=dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71aa855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "# How well does my trained model actually perform?\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "\"text-generation\", model=\"artifacts/checkpoint-200\")\n",
    "\n",
    "txt = \"Artificial Intelligence will\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1149e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Artificial Intelligence will\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80769738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Advanced Training Loop ================\n",
    "# Since I'm primarily interested in autocompletion for prompts around meaning,\n",
    "# I will give more weight to training samples with \"meaning\" and its synonyms.\n",
    "# Let's check for these words' existence in the tokenizer vocabulary\n",
    "\n",
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"connotation\",\n",
    "    \"content\",\n",
    "    \"context\",\n",
    "    \"definition\",\n",
    "    \"effect\",\n",
    "    \"essence\",\n",
    "    \"explanation\",\n",
    "    \"hint\",\n",
    "    \"implication\",\n",
    "    \"interpretation\",\n",
    "    \"nuance\",\n",
    "    \"sense\",\n",
    "    \"significance\",\n",
    "    \"spirit\",\n",
    "    \"purpose\",\n",
    "    \"direction\",\n",
    "    \"subject\",\n",
    "    \"substance\",\n",
    "    \"understanding\",\n",
    "    \"value\",\n",
    "    \"intention\",\n",
    "    \"aim\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfef24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You need to study the below code. You initially copied and pasted it into the notebook cell. \n",
    "### It's value is such that closer inspection is warranted, and will pay dividends later in the form\n",
    "### of knowledge which can be applied to future problems.\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "        axis=[0, 2]\n",
    "    )\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b748eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tr_loader, batch_size=32, shuffle=True)\n",
    "eval_dataloader = DataLoader(dev_loader, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a90c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bca8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(fp16=True)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8324c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 5000\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "#                     \"lr\": get_lr(),\n",
    "#                     \"samples\": step * samples_per_step,\n",
    "#                     \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                repo.push_to_hub(\n",
    "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547778a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "\"text-generation\", model=\"artifacts/checkpoint-200\")\n",
    "\n",
    "txt = \"Artificial intelligence\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d77b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6af95c5ed43ff950dc273fb358a3c7543a579024e645fcf2892b4462426f781d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
